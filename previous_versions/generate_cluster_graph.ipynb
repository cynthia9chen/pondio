{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "R6VHjzJ6YUZT"
      },
      "outputs": [],
      "source": [
        "# pip install sentence_transformers\n",
        "# pip install nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "kIw_p6Hy4f6M"
      },
      "outputs": [],
      "source": [
        "# 1: use sentence transformer\n",
        "# https://github.com/UKPLab/sentence-transformers/blob/master/examples/applications/clustering/kmeans.py\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.cluster import KMeans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "5pSbq_sgYXOa"
      },
      "outputs": [],
      "source": [
        "import csv\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from pandas import read_csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQJsmQNbomx3",
        "outputId": "f45f783d-a8a8-4686-ae85-92f9f64726cf"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n"
          ]
        }
      ],
      "source": [
        "# imports for NLP\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('omw-1.4')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from dateutil import parser\n",
        "import string\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation as LDA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "DdqrL93UYZSC"
      },
      "outputs": [],
      "source": [
        "history = []\n",
        "with open('key_search_terms_cynthia.csv', 'r') as csvfile:\n",
        "    datareader = csv.reader(csvfile)\n",
        "    for row in datareader:\n",
        "        # prints normalized term\n",
        "        history.append(row[3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "id": "ugx4CuY-oW67"
      },
      "outputs": [],
      "source": [
        "# NLP functions for cleaning text\n",
        "\n",
        "def remove_punctuation(input):\n",
        "  return input.translate(str.maketrans('','', string.punctuation))\n",
        "\n",
        "def remove_whitespaces(input):\n",
        "  return \" \".join(input.split())\n",
        "\n",
        "def tokenize(input):\n",
        "  return word_tokenize(input)\n",
        "\n",
        "def remove_stop_words(input):\n",
        "  input = word_tokenize(input)\n",
        "  return [word for word in input if word not in stopwords.words('english')]\n",
        "\n",
        "def lemmatize(input):\n",
        "  lemmatizer=WordNetLemmatizer()\n",
        "  input_str=word_tokenize(input)\n",
        "  new_words = []\n",
        "  for word in input_str:\n",
        "    new_words.append(lemmatizer.lemmatize(word))\n",
        "  return ' '.join(new_words)\n",
        "\n",
        "# pipeline for cleaning\n",
        "def nlp_pipeline(text):\n",
        "  return lemmatize(' '.join(remove_stop_words(remove_whitespaces(remove_punctuation(text)))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "id": "GIHWCu43r5R_"
      },
      "outputs": [],
      "source": [
        "def find_topics(word_cluster):\n",
        "    text = ' '.join(word_cluster)\n",
        "    # text = nlp_pipeline(question_body)\n",
        "    # print(\"cleaned text: \", text)\n",
        "    count_vectorizer = CountVectorizer(stop_words='english')\n",
        "    count_data = count_vectorizer.fit_transform([text])\n",
        "\n",
        "    number_of_tags = 1\n",
        "    lda = LDA(n_components=1, n_jobs=-1)\n",
        "    lda.fit(count_data)\n",
        "\n",
        "    words = count_vectorizer.get_feature_names_out()\n",
        "    # Get topics from model\n",
        "    topics = [[words[i] for i in topic.argsort()[:-number_of_tags - 1:-1]] for (topic_idx, topic) in enumerate(lda.components_)]\n",
        "\n",
        "    topics = np.array(topics).ravel()\n",
        "    return topics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "id": "5bgkfCPpqKkO"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "def clean_text(text):\n",
        "    # remove numbers\n",
        "    text_nonum = re.sub(r'\\d+', '', text)\n",
        "    # remove punctuations and convert characters to lower case\n",
        "    text_nopunct = \"\".join([char.lower() for char in text_nonum if char not in string.punctuation]) \n",
        "    # substitute multiple whitespace with single whitespace\n",
        "    # Also, removes leading and trailing whitespaces\n",
        "    text_no_doublespace = re.sub('\\s+', ' ', text_nopunct).strip()\n",
        "    return text_no_doublespace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "PxGh_7kioa7L"
      },
      "outputs": [],
      "source": [
        "history = [nlp_pipeline(h) for h in history]\n",
        "history = [clean_text(h) for h in history]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "cjZT4qsUYdfs"
      },
      "outputs": [],
      "source": [
        "# Perform kmean clustering\n",
        "# ideally set num_clusters dynamically \n",
        "\n",
        "def generate_clusters(num_clusters, phrase_list):\n",
        "  sentence_emb = embedder.encode(phrase_list)\n",
        "  clusters = {}\n",
        "\n",
        "  clustering_model = KMeans(n_clusters=num_clusters)\n",
        "  clustering_model.fit(sentence_emb)\n",
        "  cluster_assignment = clustering_model.labels_\n",
        "\n",
        "  clustered_sentences = [[] for i in range(num_clusters)]\n",
        "  for sentence_id, cluster_id in enumerate(cluster_assignment):\n",
        "      clustered_sentences[cluster_id].append(phrase_list[sentence_id])\n",
        "\n",
        "  for i, cluster in enumerate(clustered_sentences):\n",
        "    topic = find_topics(cluster)[0]\n",
        "    cluster = [i for i in cluster if i]\n",
        "    clusters[topic] = cluster\n",
        "    sizes[topic] = len(cluster)\n",
        "  return clusters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "vSaJTOLNrOix"
      },
      "outputs": [],
      "source": [
        "sizes = {}\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "roots = generate_clusters(10, history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jl-JGZ4OEmp-",
        "outputId": "8597563c-58f3-408e-85ea-2daff1a41a93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "parent belt\n",
            "parent jacket\n",
            "child thml\n",
            "parent lyric\n",
            "child imagen\n",
            "child sydel\n",
            "parent cvsz\n",
            "parent weather\n",
            "child statue\n",
            "child summer\n",
            "parent plantz\n",
            "child comedrecreationcenterataddamsparkz\n",
            "child courtyardbymarriottnearallstonbostonmaz\n",
            "parent harvard\n",
            "child graphology\n",
            "child google\n",
            "parent chicago\n",
            "child hotel\n",
            "child near\n",
            "parent python\n",
            "child module\n",
            "parent derrick\n",
            "child white\n",
            "child jenisz\n",
            "child revel\n",
            "parent imagen\n",
            "child liang\n",
            "child nuro\n",
            "parent sydel\n",
            "child electric\n",
            "child aa\n",
            "parent summer\n",
            "child festival\n",
            "parent comedrecreationcenterataddamsparkz\n",
            "child plantzdatamb\n",
            "child yoganearcambridgemaz\n",
            "parent courtyardbymarriottnearallstonbostonmaz\n",
            "child grocerywestloopz\n",
            "child quincystbrooklynnyz\n",
            "parent graphology\n",
            "child graph\n",
            "child visualization\n",
            "parent google\n",
            "child policy\n",
            "child ai\n",
            "parent hotel\n",
            "parent near\n",
            "child bar\n",
            "child place\n",
            "parent module\n",
            "child cache\n",
            "child named\n",
            "child github\n",
            "parent white\n",
            "child wife\n",
            "parent jenisz\n",
            "child brandes\n",
            "child kendallmarriottz\n",
            "parent revel\n",
            "child saladnearbyz\n",
            "child pennstationz\n",
            "parent nuro\n",
            "child celtic\n",
            "child swift\n",
            "parent plantzdatamb\n",
            "child soulcyclechicagozdatamb\n",
            "child vaticinvestmentsnewyorkzdatamb\n",
            "child urbanoutfitterszdatamb\n",
            "parent policy\n",
            "child zoom\n",
            "child trading\n",
            "parent ai\n",
            "child goodreads\n",
            "child pulse\n",
            "parent saladnearbyz\n",
            "child brunchz\n",
            "child coffeenearmez\n",
            "parent swift\n",
            "child illa\n",
            "child warrior\n",
            "parent trading\n",
            "child vatic\n",
            "child rate\n",
            "parent goodreads\n",
            "child tool\n",
            "child code\n",
            "parent coffeenearmez\n",
            "child rauschenberg\n",
            "child bobabellevuez\n",
            "parent illa\n",
            "child wok\n",
            "child van\n",
            "parent code\n",
            "child way\n",
            "child case\n"
          ]
        }
      ],
      "source": [
        "queue = []\n",
        "graph = {}\n",
        "clusters = roots\n",
        "size_thresh = 20\n",
        "num_children = 3\n",
        "\n",
        "# add original parent topics\n",
        "for topic in roots.keys():\n",
        "    if sizes[topic] >= size_thresh:\n",
        "        queue.append(topic)\n",
        "        graph[topic] = set()\n",
        "\n",
        "while len(queue) > 0:\n",
        "  parent_topic = queue.pop(0)\n",
        "  print('parent', parent_topic)\n",
        "  parent_cluster = clusters[parent_topic]\n",
        "  if len(parent_cluster) >= size_thresh and len(set(parent_cluster)) > num_children:\n",
        "    new_clusters = generate_clusters(num_children, parent_cluster)\n",
        "    for child_topic in new_clusters.keys():\n",
        "      if child_topic not in clusters.keys():\n",
        "        print('child', child_topic)\n",
        "\n",
        "        # add links between parent + child to graph\n",
        "        graph[child_topic] = set()\n",
        "        graph[child_topic].add(parent_topic)\n",
        "        graph[parent_topic].add(child_topic)\n",
        "\n",
        "        new_c = new_clusters[child_topic]\n",
        "        clusters[child_topic] = new_c\n",
        "        sizes[child_topic] = len(new_c)\n",
        "\n",
        "        # add child to queue if we re-cluster again\n",
        "        if sizes[child_topic] >= size_thresh:\n",
        "          queue.append(child_topic)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "id": "qPmnz4ayCOCF"
      },
      "outputs": [],
      "source": [
        "for k in graph.keys():\n",
        "  graph[k] = list(graph[k])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UeLSsJtXJcTM",
        "outputId": "9fe0c1d5-e7bc-4aaf-bdf5-83b1334f0e82"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'belt': [],\n",
              " 'jacket': ['thml'],\n",
              " 'lyric': ['sydel', 'imagen'],\n",
              " 'cvsz': [],\n",
              " 'weather': ['summer', 'statue'],\n",
              " 'plantz': ['courtyardbymarriottnearallstonbostonmaz',\n",
              "  'comedrecreationcenterataddamsparkz'],\n",
              " 'harvard': ['graphology', 'google'],\n",
              " 'chicago': ['near', 'hotel'],\n",
              " 'python': ['module'],\n",
              " 'derrick': ['white', 'jenisz', 'revel'],\n",
              " 'thml': ['jacket'],\n",
              " 'imagen': ['liang', 'lyric', 'nuro'],\n",
              " 'sydel': ['aa', 'lyric', 'electric'],\n",
              " 'statue': ['weather'],\n",
              " 'summer': ['festival', 'weather'],\n",
              " 'comedrecreationcenterataddamsparkz': ['plantzdatamb',\n",
              "  'plantz',\n",
              "  'yoganearcambridgemaz'],\n",
              " 'courtyardbymarriottnearallstonbostonmaz': ['plantz',\n",
              "  'quincystbrooklynnyz',\n",
              "  'grocerywestloopz'],\n",
              " 'graphology': ['harvard', 'visualization', 'graph'],\n",
              " 'google': ['harvard', 'ai', 'policy'],\n",
              " 'hotel': ['chicago'],\n",
              " 'near': ['chicago', 'place', 'bar'],\n",
              " 'module': ['python', 'cache', 'github', 'named'],\n",
              " 'white': ['derrick', 'wife'],\n",
              " 'jenisz': ['kendallmarriottz', 'derrick', 'brandes'],\n",
              " 'revel': ['pennstationz', 'derrick', 'saladnearbyz'],\n",
              " 'liang': ['imagen'],\n",
              " 'nuro': ['swift', 'imagen', 'celtic'],\n",
              " 'electric': ['sydel'],\n",
              " 'aa': ['sydel'],\n",
              " 'festival': ['summer'],\n",
              " 'plantzdatamb': ['vaticinvestmentsnewyorkzdatamb',\n",
              "  'urbanoutfitterszdatamb',\n",
              "  'soulcyclechicagozdatamb',\n",
              "  'comedrecreationcenterataddamsparkz'],\n",
              " 'yoganearcambridgemaz': ['comedrecreationcenterataddamsparkz'],\n",
              " 'grocerywestloopz': ['courtyardbymarriottnearallstonbostonmaz'],\n",
              " 'quincystbrooklynnyz': ['courtyardbymarriottnearallstonbostonmaz'],\n",
              " 'graph': ['graphology'],\n",
              " 'visualization': ['graphology'],\n",
              " 'policy': ['zoom', 'trading', 'google'],\n",
              " 'ai': ['goodreads', 'pulse', 'google'],\n",
              " 'bar': ['near'],\n",
              " 'place': ['near'],\n",
              " 'cache': ['module'],\n",
              " 'named': ['module'],\n",
              " 'github': ['module'],\n",
              " 'wife': ['white'],\n",
              " 'brandes': ['jenisz'],\n",
              " 'kendallmarriottz': ['jenisz'],\n",
              " 'saladnearbyz': ['coffeenearmez', 'revel', 'brunchz'],\n",
              " 'pennstationz': ['revel'],\n",
              " 'celtic': ['nuro'],\n",
              " 'swift': ['warrior', 'nuro', 'illa'],\n",
              " 'soulcyclechicagozdatamb': ['plantzdatamb'],\n",
              " 'vaticinvestmentsnewyorkzdatamb': ['plantzdatamb'],\n",
              " 'urbanoutfitterszdatamb': ['plantzdatamb'],\n",
              " 'zoom': ['policy'],\n",
              " 'trading': ['policy', 'vatic', 'rate'],\n",
              " 'goodreads': ['ai', 'tool', 'code'],\n",
              " 'pulse': ['ai'],\n",
              " 'brunchz': ['saladnearbyz'],\n",
              " 'coffeenearmez': ['bobabellevuez', 'rauschenberg', 'saladnearbyz'],\n",
              " 'illa': ['van', 'swift', 'wok'],\n",
              " 'warrior': ['swift'],\n",
              " 'vatic': ['trading'],\n",
              " 'rate': ['trading'],\n",
              " 'tool': ['goodreads'],\n",
              " 'code': ['case', 'way', 'goodreads'],\n",
              " 'rauschenberg': ['coffeenearmez'],\n",
              " 'bobabellevuez': ['coffeenearmez'],\n",
              " 'wok': ['illa'],\n",
              " 'van': ['illa'],\n",
              " 'way': ['code'],\n",
              " 'case': ['code']}"
            ]
          },
          "execution_count": 89,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "graph"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaaP7IpQFpKy",
        "outputId": "21fde51c-7537-45ce-a1be-b38dcda7b796"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'belt': 36,\n",
              " 'jacket': 2,\n",
              " 'lyric': 165,\n",
              " 'cvsz': 40,\n",
              " 'weather': 25,\n",
              " 'plantz': 137,\n",
              " 'harvard': 46,\n",
              " 'chicago': 131,\n",
              " 'python': 26,\n",
              " 'derrick': 137,\n",
              " 'thml': 9,\n",
              " 'imagen': 17,\n",
              " 'sydel': 11,\n",
              " 'statue': 13,\n",
              " 'summer': 13,\n",
              " 'comedrecreationcenterataddamsparkz': 30,\n",
              " 'courtyardbymarriottnearallstonbostonmaz': 18,\n",
              " 'graphology': 15,\n",
              " 'google': 42,\n",
              " 'hotel': 23,\n",
              " 'near': 6,\n",
              " 'module': 21,\n",
              " 'white': 2,\n",
              " 'jenisz': 21,\n",
              " 'revel': 11,\n",
              " 'liang': 18,\n",
              " 'nuro': 67,\n",
              " 'electric': 8,\n",
              " 'aa': 4,\n",
              " 'festival': 12,\n",
              " 'plantzdatamb': 36,\n",
              " 'yoganearcambridgemaz': 9,\n",
              " 'grocerywestloopz': 6,\n",
              " 'quincystbrooklynnyz': 1,\n",
              " 'graph': 5,\n",
              " 'visualization': 6,\n",
              " 'policy': 22,\n",
              " 'ai': 19,\n",
              " 'bar': 8,\n",
              " 'place': 14,\n",
              " 'cache': 4,\n",
              " 'named': 11,\n",
              " 'github': 6,\n",
              " 'wife': 18,\n",
              " 'brandes': 13,\n",
              " 'kendallmarriottz': 12,\n",
              " 'saladnearbyz': 12,\n",
              " 'pennstationz': 10,\n",
              " 'celtic': 10,\n",
              " 'swift': 14,\n",
              " 'soulcyclechicagozdatamb': 16,\n",
              " 'vaticinvestmentsnewyorkzdatamb': 7,\n",
              " 'urbanoutfitterszdatamb': 13,\n",
              " 'zoom': 9,\n",
              " 'trading': 15,\n",
              " 'goodreads': 10,\n",
              " 'pulse': 9,\n",
              " 'brunchz': 11,\n",
              " 'coffeenearmez': 9,\n",
              " 'illa': 5,\n",
              " 'warrior': 10,\n",
              " 'vatic': 6,\n",
              " 'rate': 2,\n",
              " 'tool': 8,\n",
              " 'code': 8,\n",
              " 'rauschenberg': 8,\n",
              " 'bobabellevuez': 8,\n",
              " 'wok': 9,\n",
              " 'van': 7,\n",
              " 'way': 6,\n",
              " 'case': 6}"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "sizes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PflLP4v3Fvwg"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "generate_cluster_graph.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.9.2 64-bit ('3.9.2')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "4c98f76eb950f64ed0b7364c1fd6335f32006725992aaa510f781af4184aaff7"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
